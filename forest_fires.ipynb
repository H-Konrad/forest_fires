{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae555f06-d3e8-4d37-8297-0b483534ae3b",
   "metadata": {
    "id": "ae555f06-d3e8-4d37-8297-0b483534ae3b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sorting import standardise, unstandardise, k_fold_splits\n",
    "from linear_regressions import prediction_error, linear_data, linear_regression, linear_analysis\n",
    "from polyn_regression import poly_data, poly_regression, poly_analysis\n",
    "from ridge_regressions import ridge_regression, pre_poly, ridge_analysis\n",
    "from gd_linear import energy_function1, gradient_descent1\n",
    "from gd_ridge import energy_function2, gradient_descent2\n",
    "from gd_huber import huber_gradient, energy_function3, huber_loss_gradient_descent\n",
    "from proximal_gd import proximal_map, proximal_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc8343-ebee-4c8c-bc19-796e0c4b5276",
   "metadata": {
    "id": "d8bc8343-ebee-4c8c-bc19-796e0c4b5276"
   },
   "outputs": [],
   "source": [
    "fires = pd.read_csv(\"forestfires.csv\")\n",
    "\n",
    "months = {\"jan\": 1., \"feb\": 2., \"mar\": 3., \"apr\": 4., \"may\": 5., \"jun\": 6., \"jul\": 7., \"aug\": 8., \"sep\": 9., \"oct\": 10., \"nov\": 11., \"dec\": 12.}\n",
    "days = {\"mon\": 1., \"tue\": 2., \"wed\": 3., \"thu\": 4., \"fri\": 5., \"sat\": 6., \"sun\": 7.}\n",
    "\n",
    "fires[\"month\"] = fires[\"month\"].map(months)\n",
    "fires[\"day\"] = fires[\"day\"].map(days)\n",
    "\n",
    "fires[\"ln_area\"] = np.log(fires[\"area\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161955c-f886-4ef0-8202-b6e56d1f2e42",
   "metadata": {
    "id": "8161955c-f886-4ef0-8202-b6e56d1f2e42",
    "outputId": "adaab6bc-ca89-4f03-b42e-05cf70e647a9"
   },
   "outputs": [],
   "source": [
    "corr = fires[[\"X\", \"Y\", \"month\", \"day\", \"FFMC\", \"DMC\", \"DC\", \"ISI\", \"temp\", \"RH\", \"wind\", \"rain\", \"area\"]].corr()\n",
    "\n",
    "fig, axes = plt.subplots(figsize = (10, 3.5))\n",
    "\n",
    "sns.heatmap(corr, cmap = \"viridis\", annot = True, fmt = \".2f\", ax = axes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b61d2-652c-4023-8a23-974f15aac6a7",
   "metadata": {
    "id": "9b5b61d2-652c-4023-8a23-974f15aac6a7",
    "outputId": "dda5db96-0a93-4a04-ce67-1a6128021e1c"
   },
   "outputs": [],
   "source": [
    "sizes = fires.groupby([\"X\", \"Y\"])[\"area\"].mean().to_dict()\n",
    "x = [i[0] for i in sizes.keys()]\n",
    "y = [i[1] for i in sizes.keys()]\n",
    "means = [((i * 100)+1) for i in sizes.values()]\n",
    "\n",
    "plt.figure(figsize = (10, 3))\n",
    "\n",
    "plt.scatter(x, y, s = means, alpha = 0.5, c = \"springgreen\", ec = \"black\")\n",
    "plt.xlabel(\"X Spatial Coordinate\")\n",
    "plt.ylabel(\"Y Spatial Coordinate\")\n",
    "plt.grid(alpha = 0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247de8a-87b6-4877-b470-f2481513860b",
   "metadata": {
    "id": "7247de8a-87b6-4877-b470-f2481513860b",
    "outputId": "01a9c1a2-bc04-4500-eaee-e044554305d4"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 6, figsize = (20, 7.5))\n",
    "\n",
    "axes[0, 0].hist(fires[\"month\"], bins = 12, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[0, 0].set_title(\"Months of the Year\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "axes[0, 0].set_xlabel(\"Month\")\n",
    "\n",
    "axes[0, 1].hist(fires[\"day\"], bins = 7, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[0, 1].set_title(\"Days of the Week\")\n",
    "axes[0, 1].set_xlabel(\"Day\")\n",
    "\n",
    "axes[0, 2].hist(fires[\"FFMC\"], bins = 30, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[0, 2].set_title(\"Fine Fuel Moisture Code\")\n",
    "axes[0, 2].set_xlabel(\"FFMC\")\n",
    "\n",
    "axes[0, 3].hist(fires[\"DMC\"], bins = 30, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[0, 3].set_title(\"Duff Moisture Code\")\n",
    "axes[0, 3].set_xlabel(\"DMC\")\n",
    "\n",
    "axes[0, 4].hist(fires[\"DC\"], bins = 30, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[0, 4].set_title(\"Drought Code\")\n",
    "axes[0, 4].set_xlabel(\"DC\")\n",
    "\n",
    "axes[0, 5].hist(fires[\"area\"], bins = 30, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[0, 5].set_title(\"Area\")\n",
    "axes[0, 5].set_xlabel(\"HA\")\n",
    "\n",
    "axes[1, 0].hist(fires[\"ISI\"], bins = 30, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[1, 0].set_title(\"Initial Spread Index\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "axes[1, 0].set_xlabel(\"ISI\")\n",
    "\n",
    "axes[1, 1].hist(fires[\"temp\"], bins = 30, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[1, 1].set_title(\"Temperature\")\n",
    "axes[1, 1].set_xlabel(\"Celsius\")\n",
    "\n",
    "axes[1, 2].hist(fires[\"RH\"], bins = 30, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[1, 2].set_title(\"Relative Humidity\")\n",
    "axes[1, 2].set_xlabel(\"%\")\n",
    "\n",
    "axes[1, 3].hist(fires[\"wind\"], bins = 30, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[1, 3].set_title(\"Wind Speed\")\n",
    "axes[1, 3].set_xlabel(\"KM/H\")\n",
    "\n",
    "axes[1, 4].hist(fires[\"rain\"], bins = 30, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[1, 4].set_title(\"Rain\")\n",
    "axes[1, 4].set_xlabel(\"MM/M^2\")\n",
    "\n",
    "axes[1, 5].hist(fires[\"ln_area\"], bins = 30, ec = \"black\", color = \"cornflowerblue\")\n",
    "axes[1, 5].set_title(\"Logarithm of Area\")\n",
    "axes[1, 5].set_xlabel(\"Log(HA+1)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d6584-59a8-4120-a1fe-d073fa2db4ac",
   "metadata": {
    "id": "f28d6584-59a8-4120-a1fe-d073fa2db4ac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "months = {b\"jan\": 1., b\"feb\": 2., b\"mar\": 3., b\"apr\": 4., b\"may\": 5., b\"jun\": 6., b\"jul\": 7., b\"aug\": 8., b\"sep\": 9., b\"oct\": 10., b\"nov\": 11.,\n",
    "          b\"dec\": 12.}\n",
    "days = {b\"mon\": 1., b\"tue\": 2., b\"wed\": 3., b\"thu\": 4., b\"fri\": 5., b\"sat\": 6., b\"sun\": 7.}\n",
    "\n",
    "converter_month = lambda x: months[x]\n",
    "converter_day = lambda x: days[x]\n",
    "converter_log = lambda x: np.log(1 + float(x))\n",
    "\n",
    "forest_fires_data = np.genfromtxt(\"forestfires.csv\", delimiter = \",\", skip_header = 1, usecols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "                                  converters = {2: converter_month, 3: converter_day, 12: converter_log})\n",
    "\n",
    "standardised_forest_fires_data, data_mean, data_std = standardise(forest_fires_data)\n",
    "k_fold_training_data, k_fold_testing_data = k_fold_splits(standardised_forest_fires_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a7b20-eddb-4734-99d7-78afb16834b6",
   "metadata": {
    "id": "214a7b20-eddb-4734-99d7-78afb16834b6",
    "outputId": "1b26c35f-749f-4a69-cdb6-c36afb6dc96d"
   },
   "outputs": [],
   "source": [
    "# linear regression\n",
    "\n",
    "linear_training_data, linear_testing_data = [], []\n",
    "for i in range(len(k_fold_training_data)):\n",
    "    linear_training_data.append((linear_data(k_fold_training_data[i][0]), k_fold_training_data[i][1]))\n",
    "    linear_testing_data.append((linear_data(k_fold_testing_data[i][0]), k_fold_testing_data[i][1]))\n",
    "\n",
    "linear_analysis(linear_training_data, linear_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad50eea-5579-4498-851f-8284e16e2b06",
   "metadata": {
    "id": "0ad50eea-5579-4498-851f-8284e16e2b06",
    "outputId": "58034dc1-b698-46b2-9a74-57c6ae464257"
   },
   "outputs": [],
   "source": [
    "# poly regression\n",
    "\n",
    "poly_analysis(k_fold_training_data, k_fold_testing_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ae215-9a57-40c3-8746-3cd1cdfb3840",
   "metadata": {
    "id": "c68ae215-9a57-40c3-8746-3cd1cdfb3840",
    "outputId": "cf1db688-283f-4489-f148-2e42bf698af8"
   },
   "outputs": [],
   "source": [
    "# ridge regression\n",
    "\n",
    "ridge_training_data, ridge_testing_data = pre_poly(k_fold_training_data, k_fold_testing_data, 25)\n",
    "\n",
    "optimal_degree, optimal_alpha, best_weights_mean, best_weights_std, best_mse_mean, best_mse_std = ridge_analysis(ridge_training_data,\n",
    "                                                                                                                 ridge_testing_data,\n",
    "                                                                                                                 np.linspace(0, 10000, 500))\n",
    "\n",
    "print(optimal_degree, optimal_alpha, best_weights_mean, best_weights_std, best_mse_mean, best_mse_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5005a8d-365f-492a-a8fa-941508e0a0ed",
   "metadata": {
    "id": "f5005a8d-365f-492a-a8fa-941508e0a0ed"
   },
   "outputs": [],
   "source": [
    "# gradient descent - linear\n",
    "\n",
    "gradient_weights1 = np.zeros(linear_training_data[0][0].shape[1])\n",
    "\n",
    "step_size1 = np.logspace(-10, -0.4, 51) # -0.3\n",
    "gd1_weights_list, gd1_mse_list = [], []\n",
    "for size in step_size1:\n",
    "    gd1_weights, gd1_mse = gradient_descent1(linear_training_data, linear_testing_data, gradient_weights1, size)\n",
    "    gd1_weights_list.append(gd1_weights)\n",
    "    gd1_mse_list.append(gd1_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c22f5-b647-405f-92de-cbb8b4b90147",
   "metadata": {
    "id": "db9c22f5-b647-405f-92de-cbb8b4b90147"
   },
   "outputs": [],
   "source": [
    "# gradient descent - ridge\n",
    "\n",
    "gradient_weights2 = np.zeros(ridge_training_data[optimal_degree][0][0].shape[1])\n",
    "\n",
    "step_size2 = np.logspace(-10, -5, 51)\n",
    "gd2_weights_list, gd2_mse_list = [], []\n",
    "for size in step_size2:\n",
    "    gd2_weights, gd2_mse = gradient_descent2(ridge_training_data[optimal_degree], ridge_testing_data[optimal_degree], gradient_weights2,\n",
    "                                             optimal_alpha, size)\n",
    "    gd2_weights_list.append(gd2_weights)\n",
    "    gd2_mse_list.append(gd2_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620549cd-ca32-4f1c-9948-1c6a3fedc436",
   "metadata": {
    "id": "620549cd-ca32-4f1c-9948-1c6a3fedc436"
   },
   "outputs": [],
   "source": [
    "# gradient descent with huber-loss lasso\n",
    "\n",
    "step_size3 = np.logspace(-10, -0.4, 51) # -0.4\n",
    "paramater = [0.1, 1, 10]\n",
    "alpha = [0.001, 0.1]\n",
    "\n",
    "mse_dict1 = {}\n",
    "for size in step_size3:\n",
    "    for a in alpha:\n",
    "        for p in paramater:\n",
    "            gd3_weights, gd3_mse = huber_loss_gradient_descent(linear_training_data, linear_testing_data, gradient_weights1, a, p, size)\n",
    "            if (a, p) not in mse_dict1:\n",
    "                mse_dict1[(a, p)] = []\n",
    "            mse_dict1[(a, p)].append(gd3_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a2eb5-943e-4ee4-951a-34641125bbad",
   "metadata": {
    "id": "334a2eb5-943e-4ee4-951a-34641125bbad",
    "outputId": "35e99fe4-15c7-4b9a-888e-5871d711eb4d"
   },
   "outputs": [],
   "source": [
    "# proximal gradient descent\n",
    "\n",
    "step_size4 = np.logspace(-10, -0.4, 51) # -0.4\n",
    "paramater = [0.1, 1, 10]\n",
    "alpha = [0.001, 0.1]\n",
    "\n",
    "mse_dict2 = {}\n",
    "for size in step_size4:\n",
    "    for a in alpha:\n",
    "        for p in paramater:\n",
    "            gd4_weights, gd4_mse = proximal_gradient_descent(linear_training_data, linear_testing_data, gradient_weights1, a, p, size)\n",
    "            if (a, p) not in mse_dict2:\n",
    "                mse_dict2[(a, p)] = []\n",
    "            mse_dict2[(a, p)].append(gd4_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6aa66-11a0-4d82-9687-898ec59ce362",
   "metadata": {
    "id": "31f6aa66-11a0-4d82-9687-898ec59ce362"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize = (20, 4))\n",
    "\n",
    "axes[0].plot(step_size1, gd1_mse_list)\n",
    "axes[0].set_title(\"Linear Gradient Descent\")\n",
    "axes[0].set_xlabel(\"Step Size\")\n",
    "axes[0].set_ylabel(\"MSE\")\n",
    "axes[0].grid(alpha = 0.1)\n",
    "\n",
    "axes[1].plot(step_size2, gd2_mse_list)\n",
    "axes[1].set_title(\"Ridge Gradient Descent\")\n",
    "axes[1].set_xlabel(\"Step Size\")\n",
    "axes[1].grid(alpha = 0.1)\n",
    "\n",
    "for key, value in mse_dict1.items():\n",
    "    axes[2].plot(step_size3, value, label = f\"(alpha, paramater):{key}\", alpha = 0.5)\n",
    "axes[2].set_title(\"Gradient Descent with Huber Loss\")\n",
    "axes[2].set_xlabel(\"Step Size\")\n",
    "axes[2].grid(alpha = 0.1)\n",
    "axes[2].legend()\n",
    "\n",
    "for key, value in mse_dict2.items():\n",
    "    axes[3].plot(step_size4, value, label = f\"alpha:{key}\", alpha = 0.5)\n",
    "axes[3].set_title(\"Proximal Gradient Descent\")\n",
    "axes[3].set_xlabel(\"Step Size\")\n",
    "axes[3].grid(alpha = 0.1)\n",
    "axes[3].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fee61-e822-41a8-be38-001865dfd3c2",
   "metadata": {
    "id": "788fee61-e822-41a8-be38-001865dfd3c2"
   },
   "outputs": [],
   "source": [
    "def prediction(inputs, weights):\n",
    "    return inputs@weights\n",
    "\n",
    "final_weights, final_mse = gradient_descent1(linear_training_data, linear_testing_data, gradient_weights1, 0.05)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(linear_training_data), figsize = (20, 4))\n",
    "\n",
    "for i in range(len(linear_training_data)):\n",
    "    stand_y_predict = prediction(linear_testing_data[i][0], final_weights)\n",
    "    unstand_y_predict = unstandardise(stand_y_predict, data_mean[12], data_std[12])\n",
    "    normal_y_predict = np.exp(unstand_y_predict) - 1\n",
    "\n",
    "    unstand_y_original = unstandardise(linear_testing_data[i][1], data_mean[12], data_std[12])\n",
    "    normal_y_original = np.exp(unstand_y_original) - 1\n",
    "\n",
    "    axes[i].plot([j for j in range(normal_y_predict.shape[0])], normal_y_predict, label = \"prediction\", c = \"springgreen\")\n",
    "    axes[i].plot([k for k in range(normal_y_original.shape[0])], normal_y_original, label = \"actual\", alpha = 0.5, c = \"tomato\")\n",
    "    axes[i].legend()\n",
    "\n",
    "axes[0].set_ylabel(\"Area\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(final_weights, final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72352235-e0f2-4e33-9b29-0bc4dc0c2f63",
   "metadata": {
    "id": "72352235-e0f2-4e33-9b29-0bc4dc0c2f63"
   },
   "outputs": [],
   "source": [
    "# second attempt\n",
    "\n",
    "cycle = np.linspace(0, np.pi, 12)\n",
    "months2 = {b\"jan\": 0., b\"feb\": np.sin(cycle[1]), b\"mar\": np.sin(cycle[2]), b\"apr\": np.sin(cycle[3]), b\"may\": np.sin(cycle[4]),\n",
    "          b\"jun\": np.sin(cycle[5]), b\"jul\": np.sin(cycle[6]), b\"aug\": np.sin(cycle[7]), b\"sep\": np.sin(cycle[8]), b\"oct\": np.sin(cycle[9]),\n",
    "          b\"nov\": np.sin(cycle[10]), b\"dec\": 0.}\n",
    "\n",
    "converter_month2 = lambda x: months2[x]\n",
    "\n",
    "forest_fires_data2 = np.genfromtxt(\"forestfires.csv\", delimiter = \",\", skip_header = 1, usecols = [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "                                  converters = {2: converter_month2, 4: converter_log, 6: converter_log, 7: converter_log, 11: converter_log,\n",
    "                                                12: converter_log})\n",
    "\n",
    "standardised_forest_fires_data2, data_mean2, data_std2 = standardise(forest_fires_data2)\n",
    "k_fold_training_data2, k_fold_testing_data2 = k_fold_splits(standardised_forest_fires_data2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2aa7c-f89d-4e94-a825-6fb860fbe6bd",
   "metadata": {
    "id": "0bc2aa7c-f89d-4e94-a825-6fb860fbe6bd"
   },
   "outputs": [],
   "source": [
    "linear_training_data2, linear_testing_data2 = [], []\n",
    "for i in range(len(k_fold_training_data2)):\n",
    "    linear_training_data2.append((linear_data(k_fold_training_data2[i][0]), k_fold_training_data2[i][1]))\n",
    "    linear_testing_data2.append((linear_data(k_fold_testing_data2[i][0]), k_fold_testing_data2[i][1]))\n",
    "\n",
    "linear_analysis(linear_training_data2, linear_testing_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43950ae6-3bc0-4e62-8c86-4346f1c8d895",
   "metadata": {
    "id": "43950ae6-3bc0-4e62-8c86-4346f1c8d895"
   },
   "outputs": [],
   "source": [
    "poly_analysis(k_fold_training_data2, k_fold_testing_data2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199f30b-e8c2-4a6f-95d1-830397cd5f3f",
   "metadata": {
    "id": "0199f30b-e8c2-4a6f-95d1-830397cd5f3f"
   },
   "outputs": [],
   "source": [
    "ridge_training_data2, ridge_testing_data2 = pre_poly(k_fold_training_data2, k_fold_testing_data2, 25)\n",
    "\n",
    "optimal_degree2, optimal_alpha2, best_weights_mean2, best_weights_std2, best_mse_mean2, best_mse_std2 = ridge_analysis(ridge_training_data2,\n",
    "                                                                                                                       ridge_testing_data2,\n",
    "                                                                                                                       np.linspace(0, 10000, 500))\n",
    "\n",
    "print(optimal_degree2, optimal_alpha2, best_weights_mean2, best_weights_std2, best_mse_mean2, best_mse_std2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a3a76-41d2-490e-bdbd-c7f686823526",
   "metadata": {
    "id": "709a3a76-41d2-490e-bdbd-c7f686823526"
   },
   "outputs": [],
   "source": [
    "gradient_weights3 = np.zeros(linear_training_data2[0][0].shape[1])\n",
    "\n",
    "step_size1 = np.logspace(-10, -0.4, 51)\n",
    "gd1_weights_list, gd1_mse_list = [], []\n",
    "for size in step_size1:\n",
    "    gd1_weights, gd1_mse = gradient_descent1(linear_training_data2, linear_testing_data2, gradient_weights3, size)\n",
    "    gd1_weights_list.append(gd1_weights)\n",
    "    gd1_mse_list.append(gd1_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280aad2-0a11-40e2-b298-50d31e0bb079",
   "metadata": {
    "id": "f280aad2-0a11-40e2-b298-50d31e0bb079"
   },
   "outputs": [],
   "source": [
    "gradient_weights4 = np.zeros(ridge_training_data2[optimal_degree2][0][0].shape[1])\n",
    "\n",
    "step_size2 = np.logspace(-10, -5, 51)\n",
    "gd2_weights_list, gd2_mse_list = [], []\n",
    "for size in step_size2:\n",
    "    gd2_weights, gd2_mse = gradient_descent2(ridge_training_data2[optimal_degree2], ridge_testing_data2[optimal_degree2], gradient_weights4,\n",
    "                                             optimal_alpha2, size)\n",
    "    gd2_weights_list.append(gd2_weights)\n",
    "    gd2_mse_list.append(gd2_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f73ff-f225-4091-8b17-2ed44735e79f",
   "metadata": {
    "id": "1b8f73ff-f225-4091-8b17-2ed44735e79f"
   },
   "outputs": [],
   "source": [
    "step_size3 = np.logspace(-10, -0.4, 51) # -0.4\n",
    "paramater = [0.1, 1, 10]\n",
    "alpha = [0.001, 0.1]\n",
    "\n",
    "mse_dict1 = {}\n",
    "for size in step_size3:\n",
    "    for a in alpha:\n",
    "        for p in paramater:\n",
    "            gd3_weights, gd3_mse = huber_loss_gradient_descent(linear_training_data2, linear_testing_data2, gradient_weights3, a, p, size)\n",
    "            if (a, p) not in mse_dict1:\n",
    "                mse_dict1[(a, p)] = []\n",
    "            mse_dict1[(a, p)].append(gd3_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb05b8-0ebc-490c-ba2d-916d6b07910b",
   "metadata": {
    "id": "c1cb05b8-0ebc-490c-ba2d-916d6b07910b"
   },
   "outputs": [],
   "source": [
    "step_size4 = np.logspace(-10, -0.4, 51) # -0.4\n",
    "paramater = [0.1, 1, 10]\n",
    "alpha = [0.001, 0.1]\n",
    "\n",
    "mse_dict2 = {}\n",
    "for size in step_size4:\n",
    "    for a in alpha:\n",
    "        for p in paramater:\n",
    "            gd4_weights, gd4_mse = proximal_gradient_descent(linear_training_data2, linear_testing_data2, gradient_weights3, a, p, size)\n",
    "            if (a, p) not in mse_dict2:\n",
    "                mse_dict2[(a, p)] = []\n",
    "            mse_dict2[(a, p)].append(gd4_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828626c-83bc-48c6-8a05-998234a1c0b9",
   "metadata": {
    "id": "6828626c-83bc-48c6-8a05-998234a1c0b9"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize = (20, 4))\n",
    "\n",
    "axes[0].plot(step_size1, gd1_mse_list)\n",
    "axes[0].set_title(\"Linear Gradient Descent\")\n",
    "axes[0].set_xlabel(\"Step Size\")\n",
    "axes[0].set_ylabel(\"MSE\")\n",
    "axes[0].grid(alpha = 0.1)\n",
    "\n",
    "axes[1].plot(step_size2, gd2_mse_list)\n",
    "axes[1].set_title(\"Ridge Gradient Descent\")\n",
    "axes[1].set_xlabel(\"Step Size\")\n",
    "axes[1].grid(alpha = 0.1)\n",
    "\n",
    "for key, value in mse_dict1.items():\n",
    "    axes[2].plot(step_size3, value, label = f\"(alpha, paramater):{key}\", alpha = 0.5)\n",
    "axes[2].set_title(\"Gradient Descent with Huber Loss\")\n",
    "axes[2].set_xlabel(\"Step Size\")\n",
    "axes[2].grid(alpha = 0.1)\n",
    "axes[2].legend()\n",
    "\n",
    "for key, value in mse_dict2.items():\n",
    "    axes[3].plot(step_size4, value, label = f\"alpha:{key}\", alpha = 0.5)\n",
    "axes[3].set_title(\"Proximal Gradient Descent\")\n",
    "axes[3].set_xlabel(\"Step Size\")\n",
    "axes[3].grid(alpha = 0.1)\n",
    "axes[3].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a3227-c0b2-4c98-a828-d53f3334d50c",
   "metadata": {
    "id": "3a4a3227-c0b2-4c98-a828-d53f3334d50c"
   },
   "outputs": [],
   "source": [
    "final_weights2, final_mse2 = gradient_descent1(linear_training_data2, linear_testing_data2, gradient_weights3, 0.05)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(linear_training_data2), figsize = (20, 4))\n",
    "\n",
    "for i in range(len(linear_training_data2)):\n",
    "    stand_y_predict = prediction(linear_testing_data2[i][0], final_weights2)\n",
    "    unstand_y_predict = unstandardise(stand_y_predict, data_mean[12], data_std[12])\n",
    "    normal_y_predict = np.exp(unstand_y_predict) - 1\n",
    "\n",
    "    unstand_y_original = unstandardise(linear_testing_data2[i][1], data_mean2[11], data_std2[11])\n",
    "    normal_y_original = np.exp(unstand_y_original) - 1\n",
    "\n",
    "    axes[i].plot([j for j in range(normal_y_predict.shape[0])], normal_y_predict, label = \"prediction\", c = \"springgreen\")\n",
    "    axes[i].plot([k for k in range(normal_y_original.shape[0])], normal_y_original, label = \"actual\", alpha = 0.5, c = \"tomato\")\n",
    "    axes[i].legend()\n",
    "\n",
    "axes[0].set_ylabel(\"Area\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(final_weights2, final_mse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb4deb-4a2c-46f2-a82c-713607c2d5b5",
   "metadata": {
    "id": "fdeb4deb-4a2c-46f2-a82c-713607c2d5b5"
   },
   "outputs": [],
   "source": [
    "fires2 = pd.read_csv(\"forestfires.csv\")\n",
    "\n",
    "def buildup_index(dmc, dc):\n",
    "    if dmc <= 0.4 * dc:\n",
    "        return 0.8 * ((dmc * dc) / (dmc + (0.4 * dc)))\n",
    "    else:\n",
    "        return dmc - (1 - ((0.8 * dc)/(dmc + (0.4 * dc))) * (0.92 + (0.0114 * dmc)**1.7))\n",
    "\n",
    "def convert_bui(bui):\n",
    "    if bui <= 80:\n",
    "        return (0.626 * (bui)**0.809) + 2\n",
    "    else:\n",
    "        return 1000 / (25 + (108.64 * (np.exp(-0.023 * bui))))\n",
    "\n",
    "def b_scale(isi, bui):\n",
    "    return 0.1 * isi * convert_bui(bui)\n",
    "\n",
    "def s_scale(isi, bui):\n",
    "    paramater = b_scale(isi, bui)\n",
    "    if paramater > 1:\n",
    "        return np.exp(2.72 * (0.434 * np.log(paramater))**0.647)\n",
    "    else:\n",
    "        return paramater\n",
    "\n",
    "fires2[\"BUI\"] = [buildup_index(dmc, dc) for dmc, dc in zip(fires2[\"DMC\"], fires2[\"DC\"])]\n",
    "fires2[\"FWI\"] = [s_scale(isi, bui) for isi, bui in zip(fires2[\"ISI\"], fires2[\"BUI\"])]\n",
    "\n",
    "fires2.to_csv(\"updated_forestfires.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290439c-4605-4292-85e8-b0169541bd5d",
   "metadata": {
    "id": "1290439c-4605-4292-85e8-b0169541bd5d"
   },
   "outputs": [],
   "source": [
    "forest_fires_data3 = np.genfromtxt(\"updated_forestfires.csv\", delimiter = \",\", skip_header = 1, usecols = [0, 2, 14, 12],\n",
    "                                  converters = {2: converter_month2, 12: converter_log})\n",
    "\n",
    "standardised_forest_fires_data3, data_mean3, data_std3 = standardise(forest_fires_data3)\n",
    "k_fold_training_data3, k_fold_testing_data3 = k_fold_splits(standardised_forest_fires_data3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89271c-feb1-47ba-a9b0-af6bfc2da2cc",
   "metadata": {
    "id": "8a89271c-feb1-47ba-a9b0-af6bfc2da2cc"
   },
   "outputs": [],
   "source": [
    "linear_training_data3, linear_testing_data3 = [], []\n",
    "for i in range(len(k_fold_training_data3)):\n",
    "    linear_training_data3.append((linear_data(k_fold_training_data3[i][0]), k_fold_training_data3[i][1]))\n",
    "    linear_testing_data3.append((linear_data(k_fold_testing_data3[i][0]), k_fold_testing_data3[i][1]))\n",
    "\n",
    "linear_analysis(linear_training_data3, linear_testing_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b95692-7d6b-4cb5-b3d5-eda7aa178711",
   "metadata": {
    "id": "38b95692-7d6b-4cb5-b3d5-eda7aa178711"
   },
   "outputs": [],
   "source": [
    "poly_analysis(k_fold_training_data3, k_fold_testing_data3, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f13d6-382a-4147-8603-48d2941c2545",
   "metadata": {
    "id": "2a5f13d6-382a-4147-8603-48d2941c2545"
   },
   "outputs": [],
   "source": [
    "ridge_training_data3, ridge_testing_data3 = pre_poly(k_fold_training_data3, k_fold_testing_data3, 25)\n",
    "\n",
    "optimal_degree3, optimal_alpha3, best_weights_mean3, best_weights_std3, best_mse_mean3, best_mse_std3 = ridge_analysis(ridge_training_data3,\n",
    "                                                                                                                       ridge_testing_data3,\n",
    "                                                                                                                       np.linspace(0, 20000, 500))\n",
    "\n",
    "print(optimal_degree3, optimal_alpha3, best_weights_mean3, best_weights_std3, best_mse_mean3, best_mse_std3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6416e40-c394-4593-a98a-42dd6a124003",
   "metadata": {
    "id": "c6416e40-c394-4593-a98a-42dd6a124003"
   },
   "outputs": [],
   "source": [
    "gradient_weights5 = np.zeros(linear_training_data3[0][0].shape[1])\n",
    "\n",
    "step_size1 = np.logspace(-10, -0.4, 51)\n",
    "gd1_weights_list, gd1_mse_list = [], []\n",
    "for size in step_size1:\n",
    "    gd1_weights, gd1_mse = gradient_descent1(linear_training_data3, linear_testing_data3, gradient_weights5, size)\n",
    "    gd1_weights_list.append(gd1_weights)\n",
    "    gd1_mse_list.append(gd1_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17dc43-e22f-4303-8015-913b4196983d",
   "metadata": {
    "id": "aa17dc43-e22f-4303-8015-913b4196983d"
   },
   "outputs": [],
   "source": [
    "gradient_weights6 = np.zeros(ridge_training_data3[optimal_degree3][0][0].shape[1])\n",
    "\n",
    "step_size2 = np.logspace(-10, -6, 51)\n",
    "gd2_weights_list, gd2_mse_list = [], []\n",
    "for size in step_size2:\n",
    "    gd2_weights, gd2_mse = gradient_descent2(ridge_training_data3[optimal_degree3], ridge_testing_data3[optimal_degree3], gradient_weights6,\n",
    "                                             optimal_alpha3, size)\n",
    "    gd2_weights_list.append(gd2_weights)\n",
    "    gd2_mse_list.append(gd2_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1108d5a-1608-4223-9d69-c54612e1b6a5",
   "metadata": {
    "id": "f1108d5a-1608-4223-9d69-c54612e1b6a5"
   },
   "outputs": [],
   "source": [
    "step_size3 = np.logspace(-10, -0.4, 51) # -0.4\n",
    "paramater = [0.1, 1, 10]\n",
    "alpha = [0.001, 0.1]\n",
    "\n",
    "mse_dict1 = {}\n",
    "for size in step_size3:\n",
    "    for a in alpha:\n",
    "        for p in paramater:\n",
    "            gd3_weights, gd3_mse = huber_loss_gradient_descent(linear_training_data3, linear_testing_data3, gradient_weights5, a, p, size)\n",
    "            if (a, p) not in mse_dict1:\n",
    "                mse_dict1[(a, p)] = []\n",
    "            mse_dict1[(a, p)].append(gd3_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b8f7b-45fc-4fca-980d-59ae3f5a5915",
   "metadata": {
    "id": "188b8f7b-45fc-4fca-980d-59ae3f5a5915"
   },
   "outputs": [],
   "source": [
    "step_size4 = np.logspace(-10, -0.4, 51) # -0.4\n",
    "paramater = [0.1, 1, 10]\n",
    "alpha = [0.001, 0.1]\n",
    "\n",
    "mse_dict2 = {}\n",
    "for size in step_size4:\n",
    "    for a in alpha:\n",
    "        for p in paramater:\n",
    "            gd4_weights, gd4_mse = proximal_gradient_descent(linear_training_data3, linear_testing_data3, gradient_weights5, a, p, size)\n",
    "            if (a, p) not in mse_dict2:\n",
    "                mse_dict2[(a, p)] = []\n",
    "            mse_dict2[(a, p)].append(gd4_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88620ec-f40b-4242-b6a4-af877198a5aa",
   "metadata": {
    "id": "f88620ec-f40b-4242-b6a4-af877198a5aa"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize = (20, 4))\n",
    "\n",
    "axes[0].plot(step_size1, gd1_mse_list)\n",
    "axes[0].set_title(\"Linear Gradient Descent\")\n",
    "axes[0].set_xlabel(\"Step Size\")\n",
    "axes[0].set_ylabel(\"MSE\")\n",
    "axes[0].grid(alpha = 0.1)\n",
    "\n",
    "axes[1].plot(step_size2, gd2_mse_list)\n",
    "axes[1].set_title(\"Ridge Gradient Descent\")\n",
    "axes[1].set_xlabel(\"Step Size\")\n",
    "axes[1].grid(alpha = 0.1)\n",
    "\n",
    "for key, value in mse_dict1.items():\n",
    "    axes[2].plot(step_size3, value, label = f\"(alpha, paramater):{key}\", alpha = 0.5)\n",
    "axes[2].set_title(\"Gradient Descent with Huber Loss\")\n",
    "axes[2].set_xlabel(\"Step Size\")\n",
    "axes[2].grid(alpha = 0.1)\n",
    "axes[2].legend()\n",
    "\n",
    "for key, value in mse_dict2.items():\n",
    "    axes[3].plot(step_size4, value, label = f\"alpha:{key}\", alpha = 0.5)\n",
    "axes[3].set_title(\"Proximal Gradient Descent\")\n",
    "axes[3].set_xlabel(\"Step Size\")\n",
    "axes[3].grid(alpha = 0.1)\n",
    "axes[3].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff3155-3bf4-4aaa-bbb8-f50e02f72af9",
   "metadata": {
    "id": "10ff3155-3bf4-4aaa-bbb8-f50e02f72af9"
   },
   "outputs": [],
   "source": [
    "final_weights3, final_mse3 = gradient_descent2(ridge_training_data3[optimal_degree3], ridge_testing_data3[optimal_degree3], gradient_weights6,\n",
    "                                               optimal_alpha3, 1.0e-06)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(ridge_training_data3[optimal_degree3]), figsize = (20, 3.5))\n",
    "\n",
    "for i in range(len(linear_training_data3)):\n",
    "    stand_y_predict = prediction(ridge_testing_data3[optimal_degree3][i][0], final_weights3)\n",
    "    unstand_y_predict = unstandardise(stand_y_predict, data_mean3[3], data_std3[3])\n",
    "    normal_y_predict = np.exp(unstand_y_predict) - 1\n",
    "\n",
    "    unstand_y_original = unstandardise(ridge_testing_data3[optimal_degree3][i][1], data_mean3[3], data_std3[3])\n",
    "    normal_y_original = np.exp(unstand_y_original) - 1\n",
    "\n",
    "    axes[i].plot([j for j in range(normal_y_predict.shape[0])], normal_y_predict, label = \"prediction\", c = \"springgreen\")\n",
    "    axes[i].plot([k for k in range(normal_y_original.shape[0])], normal_y_original, label = \"actual\", alpha = 0.5, c = \"tomato\")\n",
    "    axes[i].legend()\n",
    "\n",
    "axes[0].set_ylabel(\"Area\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"prediction3.png\")\n",
    "plt.show()\n",
    "\n",
    "print(final_weights3, final_mse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612dbd3-8c09-4bfd-8799-d4b6aabffc6b",
   "metadata": {
    "id": "8612dbd3-8c09-4bfd-8799-d4b6aabffc6b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
